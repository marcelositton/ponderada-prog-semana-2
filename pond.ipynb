
    precision = tp / (tp + fp + 1e-7)
    recall    = tp / (tp + fn + 1e-7)
    return 2.0 * precision * recall / (precision + recall + 1e-7)

# Modelo Perceptron (1 neurônio com sigmoid)
model = keras.Sequential([
    keras.Input(shape=(num_words,)),
    layers.Dense(1, activation="sigmoid")
])

# Compilação do modelo
model.compile(
    optimizer="adam",
    loss="binary_crossentropy",
    metrics=["accuracy", f1_metric]
)

model.summary()

# Treinamento
history = model.fit(
    X_train, y_train,
    epochs=50,
    batch_size=10,
    validation_split=0.2,
    verbose=2
)

# Avaliação no teste e métricas adicionais
print("\n[Evaluate] Métricas no conjunto de teste (Keras):")
test_loss, test_acc, test_f1 = model.evaluate(X_test, y_test, verbose=0)
print(f"Loss: {test_loss:.4f} | Accuracy: {test_acc:.4f} | F1: {test_f1:.4f}")

y_prob = model.predict(X_test, verbose=0).ravel()
y_pred = (y_prob >= 0.5).astype(int)

print("\n[Métricas sklearn no teste]:")
print(f"Accuracy: {accuracy_score(y_test, y_pred):.4f} | F1: {f1_score(y_test, y_pred):.4f}")

print("\nMatriz de confusão:")
print(confusion_matrix(y_test, y_pred))

print("\nRelatório de classificação:")
print(classification_report(y_test, y_pred, digits=4))
